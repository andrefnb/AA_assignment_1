Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. In case of doubt, you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.

QUESTIONS:

Q1: Considering the data provided, explain the need to standardise the attribute values.
R1: The data contained four features: variance (which varies between -3.873 and 3.681 in the training set) skewness(which varies between -53.11 and 33.831 in the training set), curtosis of Wavelet Transformed image (which varies between -19.65 and 30.201 in the training set), and the entropy of the bank note image (which varies -3.933 between  and 6.034 in the training set). These different scales might cause problems to the solver, so to avoid this we first put all values in similar scales.


Q2: Explain how you calculated the parameters for standardisation and how you used them in the test set.
R2: We calculated the mean and standard deviation for each feature of the training set, then subtracted the mean and divided the result by the standard deviation to each example of the test set.


Q3: Explain how you calculated the prior probability of an example belonging to a class (the probability before taking into account the attribute values ​​of the example) in your Naïve Bayes classifier implementation. You may include a relevant piece of your code if this helps you explain.
R3: The prior probability of an example belonging to a class is calculated taking the number of examples in the training set that belong to the class and dividing it by the total number of examples in this set.


Q4: Explain how your Naïve Bayes classifier predicts the class to which a test example belongs. You may include a relevant piece of your code if this helps you explain.
R4: Using the training set we calculated the two classes' prior probabilities and the eight different kde's (one for each feature per class), storing them for later use in the test set. These values and distributions are then used by the predict function. The function will first calculate the scores (log of the joint probabilities) for each class and feature in the set using the stored kde's. It will then sum the prior probabilities and the scores by observation for each class, storing these values in two lists, naturally with the same length as the data set. It will then compare the value of these lists for each observation and choose the largest one, that is, if the value for class 0 is larger than the one for class 1, then the classifier predicts that the observation belongs to class 0, and vice versa.


Q5: Explain the effect of the bandwidth parameter on your classifier.
R5: To train our classifier, we determined the conditional probability distribution of each feature given the class with Kernel Density Estimation. The bandwidth parameter determines the width of the kernel function, therefore different values for this parameter lead to different classifiers. The larger the bandwidth parameter, the more points the estimator takes into consideration when estimating the distribution around a value and therefore the smoother the distribution. The rougher the distribution, the more overfitting will take place, as the distribution will be more attached to the training data.


Q6: Explain what effect the gamma parameter has on the SVM classifier.
R6: The gamma parameter regulates how tightly the decision frontier will conform to the classes. The lower the gamma the broader the margin will become, conforming more loosely on the data thus smoothing the frontier. The higher the gamma the tighter the margins will be and then the decision frontier will separate more clearly between the classes.


Q7: Explain how you determined the best bandwidth and gamma parameters for your classifier and the SVM classifier. You may include a relevant piece of your code if this helps you explain.
R7: The best bandwidth and gamma parameters for the classifiers were determined using cross-validation. Cross-validation allows to select the hypothesis with the smallest validation error by averaging the error estimates over several repetitions. We partitioned the data into 5 disjoint folds. Then we trained the model with all folds but one, validate on the fold that was left out and repeat for all the folds. In the end we averaged the validation error. We then chose the model with the parameter that minimizes this error, determining the best parameter.


Q8: Explain how you obtained the best hypothesis for each classifier after optimizing all parameters.
R8: After optimizing all parameters with cross-validation, we train a model with the best parameters. This process includes the instantiation of a new classifier object and then execute the fit function with the training set. In the Naïve Bayes case this means calculating the prior probabilities again and generating and fitting new kde's for each class and feature.


Q9: Show the best parameters, the estimate of the true error for each hypothesis you obtained (your classifier and the two provided by the library), the ranges in the expected number of errors given by the approximate normal test, the McNemar test values, and discuss what you can conclude from this.
R9: For our own implementation of the Naïve Bayes classifier, the best parameter (bandwidth) is 0.24. This model estimates the true error to be 0.06095. The range in the expected number of errors given by the approximate normal test is: [59.44199,92.55801]. The Gaussian Naïve Bayes classifier in scikit-learn, on the other hand, gives an estimate of the true error of 0.09463, and the range in the expected number of errors given by the approximate normal test is: [97.74134,138.25866]. The best parameter obtained for the SVM classifier was a gamma of 0.6, resulting in an estimate of the true error of 0.04250. The range in the expected number of errors given by the approximate normal test for this last classifier is: [39.03751,66.96249]. 

The values of the McNemar tests are the following: 39.00952 for comparing  GNB (sklearn) and SVM, 8.49123 for comparing NB (own) and SVM, 31.12963 for comparing GNB (sklearn) and NB (own)

Without adjusting parameters, the estimate of the true error (test error) for the Gaussian Naïve Bayes classifier in the scikit-learn class is 0.09463. For this classifier we can exclude the hypothesis that it has the same expected error rate as any of the others, as the interval obtained with the approximate normal test does not intersect with any of the others. As the McNemar tests corroborate this, we conclude that this Gaussian Naïve Bayes classifier seems to be the worst classifier for this data.

On the other hand our own implementation of the Naïve Bayes classifier using Kernel Density Estimation, and adjusting for the best bandwidth, which turned out to be 0.24, seems to perform better, with a lower estimate for the true error of 0.06095. Looking at the range in the expected number of errors given by the approximate normal test, we see that it does not intersect with the one for the GNB classifier using scikit-learn, meaning we can exclude the hypothesis of them having the same true error. This conclusion is corroborated by McNemar's test values, which is much greater than 3.84, making us exclude the hypothesis with 95% confidence.

The best classifier for this data seems to be the SVM classifier. Although this classifier had the smallest estimate of the true error, 0.04571, when comparing to the two other classifiers, it only performs significantly different from the Gaussian Naïve Bayes classifier from scikit-learn and not from our NB classifier, when looking at the approximate normal test. The ranges in the expected number of errors of our implementation of the Naïve Bayes classifier and the SVM classifier intersect, so we cannot exclude the hypothesis that they have the same expected error rate with 95% confidence. However, if we look at McNemar's test values, we can reject this null hypothesis with the same confidence. Considering both tests we cannot confidently say that the SVM classifier is better than our implementation of the Naïve Bayes classifier.

Note that these values change every time we run the classifiers due to the shuffling of training set. These results were obtained fixing the seed of the pseudo random number generator at 600.


Q10: (Optional) Show the estimate of the true error of the optimized SVM classifier (if you did the optional part of the work) and discuss whether it was worth doing this optimization. If you did not do the optional part leave this answer blank.
R10: Fixing the same seed of the pseudo random number generator, we ran the estimate for the true error for both gamma optimized SVM and gamma and c optimized SVM. The estimate of the true error for the gamma SVM was around 0.04570 and for the gamma and c optimized SVM this estimate was around 0.04250, meaning that the estimate of the true error declined around 0.003. We performed both an approximate normal test and McNemar's test to compare the performance of the two SVM's. McNemar's test value returned 2.25 (<3.84) and the approximate normal test range of the gamma and c optimized SVM is:[42.54448, 57.0, 71.45552]. Therefore we concluded that they are not significantly different, which leads us to believe that it was not worth doing the second optimization.

